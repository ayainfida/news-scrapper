{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from deep_translator import GoogleTranslator\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "from json import dumps, loads\n",
    "from shutil import copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "    return GoogleTranslator(source='hi', target='en').translate(text=text).replace(',', '').replace('\\n', '')\n",
    "\n",
    "def create_directories(base_url, categories, label='images'):\n",
    "    # create the following dir struct; outputs > base website > categories\n",
    "    base_dir = os.path.join('output', label, urlparse(base_url).netloc)\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "\n",
    "    for category, _ in categories:\n",
    "        category_dir = os.path.join(base_dir, category)\n",
    "        if not os.path.exists(category_dir):\n",
    "            os.makedirs(category_dir)\n",
    "\n",
    "    return base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.livehindustan.com/international/news'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    main_content = soup.find('div', id='listing')\n",
    "    if main_content.find_all('a') is None:\n",
    "        main_content = soup.find('section', class_='main-wdgt listing article')\n",
    "    \n",
    "    articles_links = []\n",
    "\n",
    "    for a_tag in main_content.find_all('a'):\n",
    "        link = f\"https://www.livehindustan.com{a_tag['href']}\" if not a_tag['href'].startswith('https') else a_tag['href']\n",
    "        if link not in articles_links:\n",
    "            articles_links.append(link)\n",
    "\n",
    "    return articles_links\n",
    "\n",
    "def article_scrapper(url):\n",
    "    articles_links = get_articles_links(url)\n",
    "    data = []\n",
    "\n",
    "    def helper_scrapper(url):\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        try:\n",
    "            headline = translate(soup.find('h1').text)\n",
    "            content = str(soup)\n",
    "\n",
    "            # Split the content into lines\n",
    "            lines = content.split('\\n')\n",
    "\n",
    "            for x, line in enumerate(lines):\n",
    "                if '\"datePublished\"' in line:  # Check if the substring is in the line\n",
    "                    datetime_str = line.replace('\"datePublished\": \"', '').strip()[:-8]\n",
    "                    time = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S\")\n",
    "                if line.strip().startswith('\"image\":'):\n",
    "                    start = x\n",
    "                if line.strip().startswith('\"author\":'):\n",
    "                    end = x\n",
    "\n",
    "            for i in range(start+1, end):\n",
    "                if lines[i].strip().startswith('\"url\"'):\n",
    "                    url = lines[i].replace('\"url\":', '').replace('\"', '').replace(',', '').strip()\n",
    "                if lines[i].strip().startswith('\"caption\"'):\n",
    "                    alt = translate(lines[i].replace('\"caption\":', '').replace('\"', '').replace(',', '').strip())\n",
    "\n",
    "            return headline, time, [(alt, url)]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    for url in articles_links:\n",
    "        result = helper_scrapper(url)\n",
    "        if result is not None and len(result[2]):\n",
    "            data.append(result)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_articles(data, n=10):\n",
    "    seen_headlines = set()\n",
    "    unique_data = []\n",
    "\n",
    "    for record in data:\n",
    "        headline = record[0]\n",
    "        if headline not in seen_headlines:\n",
    "            seen_headlines.add(headline)\n",
    "            unique_data.append(record)\n",
    "\n",
    "    return sorted(unique_data, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "def download_image(img_url, save_dir, img_name):\n",
    "    try:\n",
    "        if not img_url.startswith('data:'):\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(img_url, headers=headers)\n",
    "            img_data = response.content\n",
    "            img = Image.open(BytesIO(img_data))\n",
    "            width, height = img.size\n",
    "\n",
    "            # Only save images larger than 100x100 pixels\n",
    "            if width >= 100 and height >= 100:\n",
    "                with open(os.path.join(save_dir, img_name), 'wb') as img_file:\n",
    "                    img_file.write(img_data)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def download_images(category_url, save_dir, data):\n",
    "\n",
    "    with open(os.path.join(save_dir, 'labels.csv'), 'w') as f:\n",
    "        f.write('image number,alt,article_heading\\n')\n",
    "\n",
    "    records = []\n",
    "\n",
    "    # parallising the downloads to make it faster\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "        headlines = []\n",
    "        for x, tuple in enumerate(data):\n",
    "            headline, _, images_list = tuple\n",
    "            for i, img in enumerate(images_list):\n",
    "                alt_txt, img_url = img\n",
    "                if img_url and not img_url.startswith('data:'):\n",
    "                    img_url = urljoin(category_url, img_url)\n",
    "                    combined_str = f\"{alt_txt}{headline}\".encode()\n",
    "                    img_name = f'image_{x+1}_{i+1}.jpg'\n",
    "                    records.append(f'{img_name},{alt_txt.replace(\",\", \"\")},{headline.replace(\",\", \"\")}\\n')\n",
    "                    futures.append(executor.submit(download_image, img_url, save_dir, img_name))\n",
    "\n",
    "        with open(os.path.join(save_dir, 'labels.csv'), 'a') as f:\n",
    "            f.writelines(records)\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    ('Share Market', 'https://www.livehindustan.com/business/share-market/news'),\n",
    "    ('Cricket', 'https://www.livehindustan.com/cricket/news'),\n",
    "    ('Bollywood', 'https://www.livehindustan.com/entertainment/bollywood/news'),\n",
    "    ('TV', 'https://www.livehindustan.com/entertainment/tv/news'),\n",
    "    ('Web Series', 'https://www.livehindustan.com/entertainment/web-series/news'),\n",
    "    ('Movie Review', 'https://www.livehindustan.com/entertainment/film-review/news'),\n",
    "    ('Business', 'https://www.livehindustan.com/business/news'),\n",
    "    ('Personal Investment', 'https://www.livehindustan.com/business/personal-investments/news'),\n",
    "    ('Share Market', 'https://www.livehindustan.com/business/share-market/news'),\n",
    "    ('International', 'https://www.livehindustan.com/international/news'),\n",
    "    ('Spirtuality', 'https://www.livehindustan.com/astrology/spiritual/news'),\n",
    "    ('Discourse', 'https://www.livehindustan.com/astrology/discourse/news'),\n",
    "    ('Health', 'https://www.livehindustan.com/lifestyle/health/news'),\n",
    "    ('Lifestyle', 'https://www.livehindustan.com/lifestyle/news'),\n",
    "    ('Fitness', 'https://www.livehindustan.com/lifestyle/fitness/news'),\n",
    "    ('Beauty', 'https://www.livehindustan.com/lifestyle/beauty/news'),\n",
    "    ('Food', 'https://www.livehindustan.com/lifestyle/food/news'),\n",
    "    ('Fashion', 'https://www.livehindustan.com/lifestyle/fashion/news'),\n",
    "    ('Travel', 'https://www.livehindustan.com/lifestyle/travel/news'),\n",
    "    ('Relationship', 'https://www.livehindustan.com/lifestyle/relationship/news'),\n",
    "    ('Bike', 'https://www.livehindustan.com/auto/bikes/news'),\n",
    "    ('Car', 'https://www.livehindustan.com/auto/cars/news'),\n",
    "    ('Gadgets', 'https://www.livehindustan.com/gadgets/news'),\n",
    "    ('Apps', 'https://www.livehindustan.com/gadgets/apps/news')\n",
    "]\n",
    "\n",
    "base_url = 'https://www.livehindustan.com/'\n",
    "\n",
    "base_dir = create_directories(base_url, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.livehindustan.com/viral-news/tanker-sunk-into-road-pit-in-pune-seen-diving-201726848863356.html'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline = translate(soup.find('h1').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A tanker got stuck in a pothole in Pune and drowned in the drain water; residents said- what about our safety'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-20 21:48:58\n"
     ]
    }
   ],
   "source": [
    "content = str(soup)\n",
    "\n",
    "# Split the content into lines\n",
    "lines = content.split('\\n')\n",
    "\n",
    "for x, line in enumerate(lines):\n",
    "    if '\"datePublished\"' in line:  # Check if the substring is in the line\n",
    "        datetime_str = line.replace('\"datePublished\": \"', '').strip()[:-8]\n",
    "        time = datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S\")\n",
    "    if line.strip().startswith('\"image\":'):\n",
    "        start = x\n",
    "    if line.strip().startswith('\"author\":'):\n",
    "        end = x\n",
    "\n",
    "for i in range(start+1, end):\n",
    "    if lines[i].strip().startswith('\"url\"'):\n",
    "        url = lines[i].replace('\"url\":', '').replace('\"', '').replace(',', '').strip()\n",
    "    if lines[i].strip().startswith('\"caption\"'):\n",
    "        alt = translate(lines[i].replace('\"caption\":', '').replace('\"', '').replace(',', '').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, category_url in tqdm(categories, desc='Downloading images for every category'):\n",
    "    category_dir = os.path.join(base_dir, category)\n",
    "    try:\n",
    "        data = article_scrapper(category_url)\n",
    "        download_images(category_url, category_dir, get_latest_articles(data))\n",
    "    except:\n",
    "        print(category)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(base_dir):\n",
    "    for category in os.listdir(base_dir):\n",
    "        try:\n",
    "            df = pd.read_csv(f'{base_dir}{category}/labels.csv')\n",
    "            n, _ = df.shape\n",
    "\n",
    "            pairs = []\n",
    "\n",
    "            for i in range(n):\n",
    "                for j in range(i + 1, n):\n",
    "                    article_1 = int(re.search(r'\\d+(?=_|$)',list(df.iloc[i])[0]).group())\n",
    "                    article_2 = int(re.search(r'\\d+(?=_|$)',list(df.iloc[j])[0]).group())\n",
    "                    if article_1 != article_2:\n",
    "                        pairs.append((list(df.iloc[i]), list(df.iloc[j])))\n",
    "\n",
    "            with open(f'{base_dir.split(\"/\")[2]}_pairs_{category}.csv', 'w') as f:\n",
    "                for i, pair in enumerate(pairs):\n",
    "                    p1, p2 = pair\n",
    "                    img1, alt1, headline1 = p1\n",
    "                    img2, alt2, headline2 = p2\n",
    "                    f.write(f'{i+1},{headline1},{headline2}\\n')\n",
    "                    f.write(f',https://raw.githubusercontent.com/ayainfida/news-scrapper/main/output/images/www.livehindustan.com/{urllib.parse.quote(category)}/{img1},https://raw.githubusercontent.com/ayainfida/news-scrapper/main/output/images/www.livehindustan.com/{urllib.parse.quote(category)}/{img2}\\n')\n",
    "                    f.write(f',{alt1},{alt2}\\n')\n",
    "\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file('output/images/www.livehindustan.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs = []\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.startswith('www.livehindustan.com_pairs'):\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            num_pairs.append((int(list(df.iloc[-3])[0]), file))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_array = sorted(num_pairs, key=lambda x: x[0])\n",
    "sorted_array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
