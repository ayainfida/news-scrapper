{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from io import BytesIO\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from json import dumps, loads\n",
    "from shutil import copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories(base_url, categories, label='images'):\n",
    "    # create the following dir struct; outputs > base website > categories\n",
    "    base_dir = os.path.join('output', label, urlparse(base_url).netloc)\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir)\n",
    "    \n",
    "    for category, _ in categories:\n",
    "        category_dir = os.path.join(base_dir, category)\n",
    "        if not os.path.exists(category_dir):\n",
    "            os.makedirs(category_dir)\n",
    "\n",
    "    return base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    main_tag = soup.find('main', class_='main-content')\n",
    "    info_headers = main_tag.find_all('header', class_='info-header')\n",
    "    articles_links = []\n",
    "\n",
    "    for header in info_headers:\n",
    "        a_tag = header.find(class_='title').find('a')['href']\n",
    "        link = a_tag if a_tag.startswith('https') else f'https://www.foxnews.com{a_tag}'\n",
    "        articles_links.append(link)\n",
    "    \n",
    "    return articles_links\n",
    "\n",
    "def article_scrapper(url):\n",
    "    articles_links = get_articles_links(url)\n",
    "    data = []\n",
    "\n",
    "    def helper_scrapper(url):   \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        try:\n",
    "            time = datetime.strptime(soup.find('time').text.split('EDT')[-2].strip(), \"%B %d, %Y %I:%M%p\")\n",
    "            # time = soup.find('time').text.split('EDT')[-2].strip()\n",
    "            headline = soup.find('h1').text\n",
    "            content = soup.find('div', class_='article-body')\n",
    "            images = [(img['alt'], img['src']) for img in content.find_all('img')]\n",
    "\n",
    "            return headline, time, images\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    for url in articles_links:   \n",
    "        result = helper_scrapper(url)\n",
    "        if result is not None:\n",
    "            data.append(result)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_articles(data, n=10):\n",
    "    seen_headlines = set()\n",
    "    unique_data = []\n",
    "\n",
    "    for record in data:\n",
    "        headline = record[0]\n",
    "        if headline not in seen_headlines:\n",
    "            seen_headlines.add(headline)\n",
    "            unique_data.append(record)\n",
    "\n",
    "    return sorted(unique_data, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "def download_image(img_url, save_dir, img_name):\n",
    "    try:\n",
    "        if not img_url.startswith('data:'):\n",
    "            response = requests.get(img_url)\n",
    "            img_data = response.content\n",
    "            img = Image.open(BytesIO(img_data))\n",
    "            width, height = img.size\n",
    "\n",
    "            # Only save images larger than 100x100 pixels\n",
    "            if width >= 100 and height >= 100:\n",
    "                with open(os.path.join(save_dir, img_name), 'wb') as img_file:\n",
    "                    img_file.write(img_data)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def download_images(category_url, save_dir, data):\n",
    "\n",
    "    with open(os.path.join(save_dir, 'labels.csv'), 'w') as f:\n",
    "        f.write('image number,alt,article_heading\\n')\n",
    "    \n",
    "    records = []\n",
    "\n",
    "    # parallising the downloads to make it faster\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "        headlines = []\n",
    "        for x, tuple in enumerate(data):\n",
    "            headline, _, images_list = tuple\n",
    "            for i, img in enumerate(images_list):\n",
    "                alt_txt, img_url = img\n",
    "                if alt_txt.startswith('Fox News'):\n",
    "                    continue\n",
    "                if img_url and not img_url.startswith('data:'):\n",
    "                    img_url = urljoin(category_url, img_url)\n",
    "                    combined_str = f\"{alt_txt}{headline}\".encode()\n",
    "                    img_name = f'image_{x+1}{i+1}.jpg'\n",
    "                    records.append(f'{img_name},{alt_txt.replace(\",\", \"\")},{headline.replace(\",\", \"\")}\\n')\n",
    "                    futures.append(executor.submit(download_image, img_url, save_dir, img_name))\n",
    "        \n",
    "        with open(os.path.join(save_dir, 'labels.csv'), 'a') as f:\n",
    "            f.writelines(records)\n",
    "            \n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    (\"Business\", \"https://www.foxnews.com/category/newsedge/business\"),\n",
    "    (\"Sports\", \"https://www.foxnews.com/sports\"),\n",
    "    (\"Entertainment\", \"https://www.foxnews.com/entertainment\"),\n",
    "    (\"Science\", \"https://www.foxnews.com/science\"),\n",
    "    (\"World\", \"https://www.foxnews.com/world\")\n",
    "]\n",
    "\n",
    "\n",
    "base_url = 'https://www.foxnews.com/'\n",
    "\n",
    "base_dir = create_directories(base_url, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading images for every category: 100%|██████████| 5/5 [01:42<00:00, 20.40s/it]\n"
     ]
    }
   ],
   "source": [
    "for category, category_url in tqdm(categories, desc='Downloading images for every category'):\n",
    "    try:\n",
    "        category_dir = os.path.join(base_dir, category)\n",
    "        data = article_scrapper(f'{base_url}{category}')\n",
    "        download_images(category_url, category_dir, get_latest_articles(data))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "# Create a new Excel workbook\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "# Open the CSV file\n",
    "csv_file = 'labels.csv'  # Replace with your CSV file path\n",
    "image_folder = 'output/images/www.foxnews.com/business/'  # Replace with the folder where images are stored\n",
    "\n",
    "# Read CSV data and write to Excel\n",
    "with open(csv_file, newline='', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    header = next(reader)\n",
    "    ws.append(header)  # Write header to Excel\n",
    "\n",
    "    for row in reader:\n",
    "        ws.append(row)  # Write the CSV row to Excel\n",
    "        \n",
    "        # The image file name is in the first column\n",
    "        image_path = os.path.join(image_folder, row[0])\n",
    "        \n",
    "        # Check if the image exists\n",
    "        if os.path.exists(image_path):\n",
    "            # Resize image to fit within a cell (Optional)\n",
    "            img = PILImage.open(image_path)\n",
    "            img.thumbnail((372, 238))  # Resize the image\n",
    "\n",
    "            resized_image_path = f'temp/resized_{row[0]}'\n",
    "            img.save(resized_image_path)\n",
    "\n",
    "            # Insert the image into the Excel sheet\n",
    "            img_to_insert = Image(resized_image_path)\n",
    "            ws.add_image(img_to_insert, f'A{ws.max_row}')  # Insert at the current row (A column)\n",
    "        else:\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "\n",
    "# Save the Excel file\n",
    "wb.save('output_with_images.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('labels.csv')\n",
    "n, _ = df.shape\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        pairs.append((list(df.iloc[i]), list(df.iloc[j])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(base_dir):\n",
    "    for category in os.listdir(base_dir):\n",
    "        if len(os.listdir(f'{base_dir}{category}')) <= 1:\n",
    "            continue\n",
    "        try:\n",
    "            df = pd.read_csv(f'{base_dir}{category}/labels.csv')\n",
    "            n, _ = df.shape\n",
    "\n",
    "            pairs = []\n",
    "\n",
    "            for i in range(n):\n",
    "                for j in range(i + 1, n):\n",
    "                    pairs.append((list(df.iloc[i]), list(df.iloc[j])))\n",
    "\n",
    "            with open(f'www.foxnews.com_pairs_{category}.csv', 'w') as f:\n",
    "                for i, pair in enumerate(pairs):\n",
    "                    p1, p2 = pair\n",
    "                    img1, alt1, headline1 = p1\n",
    "                    img2, alt2, headline2 = p2\n",
    "                    f.write(f'{i+1},{headline1},{headline2}\\n')\n",
    "                    f.write(f',https://raw.githubusercontent.com/ayainfida/news-scrapper/main/output/images/www.foxnews.com/{category}/{img1},https://raw.githubusercontent.com/ayainfida/news-scrapper/main/output/images/www.foxnews.com/{category}/{img2}\\n')\n",
    "                    f.write(f',{alt1},{alt2}\\n')\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file('output/images/www.foxnews.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs = []\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file == 'output' or file.startswith('pairs'):\n",
    "        continue\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        num_pairs.append((int(list(df.iloc[-3])[0]), file))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(276, 'www.foxnews.com_pairs_Executive.csv'),\n",
       " (351, 'www.foxnews.com_pairs_Crime.csv'),\n",
       " (406, 'www.foxnews.com_pairs_Immigration.csv'),\n",
       " (496, 'www.foxnews.com_pairs_Golf.csv'),\n",
       " (703, 'www.foxnews.com_pairs_Faith.csv'),\n",
       " (703, 'www.foxnews.com_pairs_Education.csv'),\n",
       " (703, 'www.foxnews.com_pairs_Disasters.csv'),\n",
       " (820, 'www.foxnews.com_pairs_World.csv'),\n",
       " (861, 'www.foxnews.com_pairs_Environment.csv'),\n",
       " (946, 'www.foxnews.com_pairs_Economy.csv'),\n",
       " (990, 'www.foxnews.com_pairs_House.csv'),\n",
       " (1378, 'www.foxnews.com_pairs_Business.csv'),\n",
       " (2926, 'www.foxnews.com_pairs_Entertainment.csv')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_array = sorted(num_pairs, key=lambda x: x[0])\n",
    "sorted_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "cat = []\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.startswith('www.foxnews.com') and file.endswith('.csv'):\n",
    "        cat.append(file.replace('.csv', '').split('_')[-1])\n",
    "\n",
    "for dir in os.listdir('output/images/www.foxnews.com'):\n",
    "    if dir not in cat:\n",
    "        shutil.rmtree(f'output/images/www.foxnews.com/{dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
