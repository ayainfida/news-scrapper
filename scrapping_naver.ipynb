{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install deep_translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB8u5pywiMla",
        "outputId": "3d48527b-9c0d-4edf-eb8b-afb812c2dddc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep_translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep_translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2024.8.30)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep_translator\n",
            "Successfully installed deep_translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmb2lOgEiP8s",
        "outputId": "8733b350-05bf-4b26-d3fa-082cd196d27d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "naioFlPJiCNs"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from io import BytesIO\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "from deep_translator import GoogleTranslator\n",
        "from datetime import datetime\n",
        "import urllib.parse\n",
        "from json import dumps, loads\n",
        "from shutil import copy2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/research-similarity/Scraping')"
      ],
      "metadata": {
        "id": "SvsWAELRiEYU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LIyJzOfKiCNv"
      },
      "outputs": [],
      "source": [
        "def translate(text):\n",
        "    return GoogleTranslator(source='ko', target='en').translate(text=text).replace(',', '').replace('\\n', '')\n",
        "\n",
        "def create_directories(base_url, categories, label='images'):\n",
        "    # create the following dir struct; outputs > base website > categories\n",
        "    base_dir = os.path.join('output', label, urlparse(base_url).netloc)\n",
        "    if not os.path.exists(base_dir):\n",
        "        os.makedirs(base_dir)\n",
        "\n",
        "    for category, _ in categories:\n",
        "        category_dir = os.path.join(base_dir, category)\n",
        "        if not os.path.exists(category_dir):\n",
        "            os.makedirs(category_dir)\n",
        "\n",
        "    return base_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cihL0kH-iCNv"
      },
      "outputs": [],
      "source": [
        "def get_articles_links(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'lxml')\n",
        "    main_content = soup.find('div', id='newsct')\n",
        "    articles_links = []\n",
        "\n",
        "    for div in main_content.find_all('div', class_='sa_text'):\n",
        "        a_tag = div.find('a')\n",
        "        link = a_tag['href']\n",
        "        if link not in articles_links:\n",
        "            articles_links.append(link)\n",
        "\n",
        "    return articles_links\n",
        "\n",
        "def article_scrapper(url):\n",
        "    articles_links = get_articles_links(url)\n",
        "    data = []\n",
        "\n",
        "    def helper_scrapper(url):\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'lxml')\n",
        "        try:\n",
        "            datetime_str = soup.find('span', class_='media_end_head_info_datestamp_time _ARTICLE_DATE_TIME').get('data-date-time')\n",
        "            time = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')\n",
        "            headline = translate(soup.find('h2').text)\n",
        "            article_content = soup.find('div', class_='newsct')\n",
        "\n",
        "            images = []\n",
        "\n",
        "            for img in article_content.find_all('img'):\n",
        "                if img.get('id'):\n",
        "                    img_alt = translate(img.get('alt', ''))  # Safely get the 'alt' attribute\n",
        "                    img_src = img.get('data-src', '')  # Safely get the 'src' attribute\n",
        "\n",
        "                    if (img_alt, img_src) not in images:\n",
        "                        images.append((img_alt, img_src))\n",
        "\n",
        "            return headline, time, images\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    for url in articles_links:\n",
        "        result = helper_scrapper(url)\n",
        "        if result is not None and len(result[2]):\n",
        "            data.append(result)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9qEajereiCNv"
      },
      "outputs": [],
      "source": [
        "def get_latest_articles(data, n=10):\n",
        "    seen_headlines = set()\n",
        "    unique_data = []\n",
        "\n",
        "    for record in data:\n",
        "        headline = record[0]\n",
        "        if headline not in seen_headlines:\n",
        "            seen_headlines.add(headline)\n",
        "            unique_data.append(record)\n",
        "\n",
        "    return sorted(unique_data, key=lambda x: x[1], reverse=True)[:n]\n",
        "\n",
        "def download_image(img_url, save_dir, img_name):\n",
        "    try:\n",
        "        if not img_url.startswith('data:'):\n",
        "            response = requests.get(img_url)\n",
        "            img_data = response.content\n",
        "            img = Image.open(BytesIO(img_data))\n",
        "            width, height = img.size\n",
        "\n",
        "            # Only save images larger than 100x100 pixels\n",
        "            if width >= 100 and height >= 100:\n",
        "                with open(os.path.join(save_dir, img_name), 'wb') as img_file:\n",
        "                    img_file.write(img_data)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "def download_images(category_url, save_dir, data):\n",
        "\n",
        "    with open(os.path.join(save_dir, 'labels.csv'), 'w') as f:\n",
        "        f.write('image number,alt,article_heading\\n')\n",
        "\n",
        "    records = []\n",
        "\n",
        "    # parallising the downloads to make it faster\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = []\n",
        "        headlines = []\n",
        "        for x, tuple in enumerate(data):\n",
        "            headline, _, images_list = tuple\n",
        "            for i, img in enumerate(images_list):\n",
        "                alt_txt, img_url = img\n",
        "                if img_url and not img_url.startswith('data:'):\n",
        "                    img_url = urljoin(category_url, img_url)\n",
        "                    combined_str = f\"{alt_txt}{headline}\".encode()\n",
        "                    img_name = f'image_{x+1}_{i+1}.jpg'\n",
        "                    records.append(f'{img_name},{alt_txt.replace(\",\", \"\")},{headline.replace(\",\", \"\")}\\n')\n",
        "                    futures.append(executor.submit(download_image, img_url, save_dir, img_name))\n",
        "\n",
        "        with open(os.path.join(save_dir, 'labels.csv'), 'a') as f:\n",
        "            f.writelines(records)\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            future.result()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nbiZSecKiCNw"
      },
      "outputs": [],
      "source": [
        "categories = [\n",
        "    (\"Politics\", \"https://news.naver.com/section/100\"),\n",
        "    (\"Economy\", \"https://news.naver.com/section/101\"),\n",
        "    (\"Society\", \"https://news.naver.com/section/102\"),\n",
        "    (\"Life & Culture\", \"https://news.naver.com/section/103\"),\n",
        "    (\"IT & Science\", \"https://news.naver.com/section/105\"),\n",
        "    (\"World\", \"https://news.naver.com/section/104\"),\n",
        "]\n",
        "\n",
        "base_url = 'https://news.naver.com/'\n",
        "\n",
        "base_dir = create_directories(base_url, categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAzePSzAiCNw",
        "outputId": "363694b8-6d08-4e19-a187-9708a94251c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading images for every category: 100%|██████████| 6/6 [04:42<00:00, 47.13s/it]\n"
          ]
        }
      ],
      "source": [
        "for category, category_url in tqdm(categories, desc='Downloading images for every category'):\n",
        "    try:\n",
        "        category_dir = os.path.join(base_dir, category)\n",
        "        data = article_scrapper(category_url)\n",
        "        download_images(category_url, category_dir, get_latest_articles(data))\n",
        "    except:\n",
        "        print(category)\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_file(base_dir):\n",
        "    for category in os.listdir(base_dir):\n",
        "        try:\n",
        "            df = pd.read_csv(f'{base_dir}{category}/labels.csv')\n",
        "            n, _ = df.shape\n",
        "\n",
        "            pairs = []\n",
        "\n",
        "            for i in range(n):\n",
        "                for j in range(i + 1, n):\n",
        "                    article_1 = int(re.search(r'\\d+(?=_|$)',list(df.iloc[i])[0]).group())\n",
        "                    article_2 = int(re.search(r'\\d+(?=_|$)',list(df.iloc[j])[0]).group())\n",
        "                    if article_1 != article_2:\n",
        "                        pairs.append((list(df.iloc[i]), list(df.iloc[j])))\n",
        "\n",
        "            with open(f'{base_dir.split(\"/\")[2]}_pairs_{category}.csv', 'w') as f:\n",
        "                for i, pair in enumerate(pairs):\n",
        "                    p1, p2 = pair\n",
        "                    img1, alt1, headline1 = p1\n",
        "                    img2, alt2, headline2 = p2\n",
        "                    f.write(f'{i+1},{headline1},{headline2}\\n')\n",
        "                    f.write(f',https://raw.githubusercontent.com/ayainfida/news-scrapper/main/output/images/news.naver.com/{urllib.parse.quote(category)}/{img1},https://raw.githubusercontent.com/ayainfida/news-scrapper/main/output/images/news.naver.com/{urllib.parse.quote(category)}/{img2}\\n')\n",
        "                    f.write(f',{alt1},{alt2}\\n')\n",
        "\n",
        "        except:\n",
        "            continue"
      ],
      "metadata": {
        "id": "YcPJzPEcqkQ3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_file('output/images/news.naver.com/')"
      ],
      "metadata": {
        "id": "nYZHr9TRqtmp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_pairs = []\n",
        "\n",
        "for file in os.listdir():\n",
        "    if file.startswith('news.naver.com_pairs'):\n",
        "        try:\n",
        "            df = pd.read_csv(file)\n",
        "            num_pairs.append((int(list(df.iloc[-3])[0]), file))\n",
        "        except:\n",
        "            continue"
      ],
      "metadata": {
        "id": "Muj-OcTuq_0t"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_array = sorted(num_pairs, key=lambda x: x[0])\n",
        "sorted_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdnlYBIhrKi3",
        "outputId": "795cbb56-455f-4d4d-a6f3-a09ad4170ba1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(126, 'news.naver.com_pairs_IT & Science.csv'),\n",
              " (158, 'news.naver.com_pairs_Economy.csv'),\n",
              " (170, 'news.naver.com_pairs_Life & Culture.csv'),\n",
              " (187, 'news.naver.com_pairs_World.csv'),\n",
              " (271, 'news.naver.com_pairs_Society.csv'),\n",
              " (424, 'news.naver.com_pairs_Politics.csv')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}